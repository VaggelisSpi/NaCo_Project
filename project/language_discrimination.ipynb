{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "from sklearn import metrics\n",
    "from random import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_dataset import create_random_dataset\n",
    "from evolutionary_algorithm import EA\n",
    "from greedy_algorithm import GreedyAlgorithm\n",
    "from neg_sel import NegativeSelection, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_aminoacids = [ '_', 'a', 'b', 'c', 'd', 'e',\n",
    "\t'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "\t'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z' ]\n",
    "allowed = set(char_aminoacids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_data_dir = \"./data/languages/\"\n",
    "english_data_dir = languages_data_dir + \"english/\"\n",
    "input_path = english_data_dir + \"moby_dick.txt\"\n",
    "output_path = english_data_dir + \"english_6_train.txt\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "processed = ''.join([c if c in allowed else '_' for c in text])\n",
    "\n",
    "# remove any uncesseary '_' characters\n",
    "processed = re.sub(r'_+', '_', processed)\n",
    "\n",
    "length = 6\n",
    "lines = [processed[i:i+length] for i in range(0, len(processed), length)]\n",
    "\n",
    "# lines must have character amount equal to length\n",
    "lines = [line for line in lines if len(line) == length]\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set\n",
    "import random\n",
    "\n",
    "languages_data_dir = \"./data/languages/\"\n",
    "english_data_dir = languages_data_dir + \"english/\"\n",
    "input_path = english_data_dir + \"bible.txt\"\n",
    "output_path = english_data_dir + \"english_6_test.txt\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "processed = ''.join([c if c in allowed else '_' for c in text])\n",
    "\n",
    "# remove any uncesseary '_' characters\n",
    "processed = re.sub(r'_+', '_', processed)\n",
    "\n",
    "length = 6\n",
    "lines = [processed[i:i+length] for i in range(0, len(processed), length)]\n",
    "\n",
    "# lines must have character amount equal to length\n",
    "lines = [line for line in lines if len(line) == length]\n",
    "\n",
    "sample_size = 2000\n",
    "sampled_lines = random.sample(lines, sample_size)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write('\\n'.join(sampled_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_path = english_data_dir + \"english_6_train.txt\"\n",
    "self_english_data = pd.read_csv(english_train_path, header=None)\n",
    "\n",
    "sampled_english_data = create_random_dataset(df=self_english_data, n=10000, seed=42)\n",
    "sampled_english_data = sampled_english_data[0].to_list()\n",
    "\n",
    "len(sampled_english_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motifs: 387420489\n"
     ]
    }
   ],
   "source": [
    "amino_acids = \"_abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "motifs = [\"\".join(motif) for motif in product(amino_acids, repeat=6)]\n",
    "\n",
    "print(f\"Total number of motifs: {len(motifs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sampled_motifs = math.ceil(len(motifs) * 0.01)\n",
    "sampled_motifs = random.sample(motifs, num_sampled_motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test12\n",
      "Size of greedily optimized data set: 3574\n"
     ]
    }
   ],
   "source": [
    "greedy_optimizer = GreedyAlgorithm(peptides=sampled_english_data, motifs=sampled_motifs, t=3, seed=42)\n",
    "optimized_dataset = greedy_optimizer.run()\n",
    "\n",
    "with open(english_train_path + \"greedy_english_10000_1.txt\", \"w\") as f:\n",
    "    for item in optimized_dataset:\n",
    "        f.write(f\"{item}\\n\")\n",
    "        \n",
    "print(f\"Size of greedily optimized data set: {len(optimized_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
